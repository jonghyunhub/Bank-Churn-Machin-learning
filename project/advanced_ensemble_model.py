# ğŸ† Advanced Ensemble Model for Bank Churn Prediction
# Playground Series S4E1 - Production Ready Code

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import roc_auc_score, log_loss
from sklearn.base import BaseEstimator, TransformerMixin
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
import optuna
import warnings
warnings.filterwarnings('ignore')

class AdvancedFeatureEngineering(BaseEstimator, TransformerMixin):
    """ê³ ë„í™”ëœ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ íŒŒì´í”„ë¼ì¸"""

    def __init__(self):
        self.label_encoders = {}
        self.scaler = RobustScaler()
        self.feature_names = None

    def fit(self, X, y=None):
        X = X.copy()

        # ë¼ë²¨ ì¸ì½”ë”©
        categorical_cols = ['Geography', 'Gender']
        for col in categorical_cols:
            if col in X.columns:
                le = LabelEncoder()
                le.fit(X[col])
                self.label_encoders[col] = le

        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§ ì¤€ë¹„
        numeric_cols = X.select_dtypes(include=[np.number]).columns
        self.scaler.fit(X[numeric_cols])

        return self

    def transform(self, X):
        X = X.copy()

        # 1. ê¸°ë³¸ ë¼ë²¨ ì¸ì½”ë”©
        for col, le in self.label_encoders.items():
            if col in X.columns:
                X[f'{col}_encoded'] = le.transform(X[col])

        # 2. ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
        X = self._create_advanced_features(X)

        # 3. ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
        drop_cols = ['CustomerId', 'Surname', 'Geography', 'Gender']
        drop_cols = [col for col in drop_cols if col in X.columns]
        X = X.drop(columns=drop_cols)

        # 4. íŠ¹ì„±ëª… ì €ì¥
        if self.feature_names is None:
            self.feature_names = list(X.columns)

        return X

    def _create_advanced_features(self, X):
        """ê³ ê¸‰ íŠ¹ì„± ìƒì„±"""

        # ë‚˜ì´ ê´€ë ¨ íŠ¹ì„±
        X['Age_squared'] = X['Age'] ** 2
        X['Age_log'] = np.log1p(X['Age'])
        X['is_senior'] = (X['Age'] >= 60).astype(int)
        X['is_young'] = (X['Age'] <= 30).astype(int)
        X['age_group'] = pd.cut(X['Age'], bins=[0, 30, 45, 60, 100], labels=[0, 1, 2, 3])

        # ì‹ ìš©ì ìˆ˜ ê´€ë ¨ íŠ¹ì„±
        X['CreditScore_normalized'] = X['CreditScore'] / 850
        X['CreditScore_squared'] = X['CreditScore'] ** 2
        X['is_excellent_credit'] = (X['CreditScore'] >= 750).astype(int)
        X['is_poor_credit'] = (X['CreditScore'] <= 580).astype(int)

        # ì”ì•¡ ê´€ë ¨ íŠ¹ì„±
        X['has_balance'] = (X['Balance'] > 0).astype(int)
        X['Balance_log1p'] = np.log1p(X['Balance'])
        X['Balance_sqrt'] = np.sqrt(X['Balance'])
        X['is_zero_balance'] = (X['Balance'] == 0).astype(int)
        X['Balance_to_Salary_ratio'] = X['Balance'] / (X['EstimatedSalary'] + 1e-8)
        X['high_balance'] = (X['Balance'] > X['Balance'].quantile(0.75)).astype(int)

        # ì œí’ˆ ê´€ë ¨ íŠ¹ì„±
        X['single_product'] = (X['NumOfProducts'] == 1).astype(int)
        X['multiple_products'] = (X['NumOfProducts'] > 2).astype(int)
        X['products_per_tenure'] = X['NumOfProducts'] / (X['Tenure'] + 1)
        X['active_products'] = X['IsActiveMember'] * X['NumOfProducts']

        # ê¸‰ì—¬ ê´€ë ¨ íŠ¹ì„±
        X['EstimatedSalary_log'] = np.log1p(X['EstimatedSalary'])
        X['high_salary'] = (X['EstimatedSalary'] > X['EstimatedSalary'].quantile(0.75)).astype(int)
        X['salary_to_age_ratio'] = X['EstimatedSalary'] / X['Age']

        # ê³ ê° ì„¸ë¶„í™” íŠ¹ì„±
        X['wealth_score'] = (
                X['CreditScore_normalized'] * 0.3 +
                X['Balance_log1p'] / X['Balance_log1p'].max() * 0.4 +
                X['EstimatedSalary'] / X['EstimatedSalary'].max() * 0.3
        )

        X['engagement_score'] = (
                X['IsActiveMember'] * 0.4 +
                X['HasCrCard'] * 0.2 +
                (X['NumOfProducts'] / 4) * 0.4
        )

        X['risk_score'] = (
                X['is_senior'] * 0.3 +
                X['is_zero_balance'] * 0.3 +
                X['single_product'] * 0.2 +
                (1 - X['IsActiveMember']) * 0.2
        )

        # ìƒí˜¸ì‘ìš© íŠ¹ì„±
        X['age_balance_interaction'] = X['Age'] * X['Balance_log1p']
        X['credit_salary_interaction'] = X['CreditScore'] * X['EstimatedSalary_log']
        X['products_tenure_interaction'] = X['NumOfProducts'] * X['Tenure']
        X['active_balance_interaction'] = X['IsActiveMember'] * X['Balance_log1p']

        # ì§€ì—­/ì„±ë³„ ì¡°í•© (ë¼ë²¨ ì¸ì½”ë”© í›„)
        if 'Geography_encoded' in X.columns and 'Gender_encoded' in X.columns:
            X['geo_gender_combo'] = X['Geography_encoded'] * 10 + X['Gender_encoded']

        return X

class AdvancedEnsemble:
    """ê³ ê¸‰ ì•™ìƒë¸” ëª¨ë¸"""

    def __init__(self, n_folds=5, random_state=42):
        self.n_folds = n_folds
        self.random_state = random_state
        self.models = {}
        self.meta_model = None
        self.cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)

    def _get_base_models(self):
        """ë² ì´ìŠ¤ ëª¨ë¸ë“¤ ì •ì˜"""

        models = {
            'xgb': xgb.XGBClassifier(
                objective='binary:logistic',
                eval_metric='auc',
                max_depth=6,
                learning_rate=0.05,
                n_estimators=500,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=1,
                reg_lambda=1,
                random_state=self.random_state,
                verbosity=0
            ),

            'lgb': lgb.LGBMClassifier(
                objective='binary',
                metric='auc',
                boosting_type='gbdt',
                num_leaves=31,
                learning_rate=0.05,
                n_estimators=500,
                feature_fraction=0.8,
                bagging_fraction=0.8,
                bagging_freq=5,
                reg_alpha=1,
                reg_lambda=1,
                random_state=self.random_state,
                verbosity=-1
            ),

            'cat': cb.CatBoostClassifier(
                iterations=500,
                learning_rate=0.05,
                depth=6,
                l2_leaf_reg=3,
                bootstrap_type='Bayesian',
                random_seed=self.random_state,
                verbose=False
            ),

            'rf': RandomForestClassifier(
                n_estimators=300,
                max_depth=12,
                min_samples_split=5,
                min_samples_leaf=2,
                max_features='sqrt',
                bootstrap=True,
                random_state=self.random_state,
                n_jobs=-1
            ),

            'et': ExtraTreesClassifier(
                n_estimators=300,
                max_depth=12,
                min_samples_split=5,
                min_samples_leaf=2,
                max_features='sqrt',
                bootstrap=True,
                random_state=self.random_state,
                n_jobs=-1
            )
        }

        return models

    def fit(self, X, y):
        """ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨"""

        self.models = self._get_base_models()

        # êµì°¨ê²€ì¦ìœ¼ë¡œ ë©”íƒ€ íŠ¹ì„± ìƒì„±
        meta_features = np.zeros((len(X), len(self.models)))

        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X, y)):
            print(f"Training fold {fold + 1}/{self.n_folds}")

            X_train_fold = X.iloc[train_idx]
            y_train_fold = y.iloc[train_idx]
            X_val_fold = X.iloc[val_idx]

            for i, (name, model) in enumerate(self.models.items()):
                model_copy = self._clone_model(model)
                model_copy.fit(X_train_fold, y_train_fold)

                val_pred = model_copy.predict_proba(X_val_fold)[:, 1]
                meta_features[val_idx, i] = val_pred

        # ë©”íƒ€ ëª¨ë¸ í›ˆë ¨
        self.meta_model = LogisticRegression(
            C=1.0,
            random_state=self.random_state,
            max_iter=1000
        )
        self.meta_model.fit(meta_features, y)

        # ì „ì²´ ë°ì´í„°ë¡œ ë² ì´ìŠ¤ ëª¨ë¸ë“¤ ì¬í›ˆë ¨
        for name, model in self.models.items():
            print(f"Training final {name} model...")
            model.fit(X, y)

        return self

    def predict_proba(self, X):
        """ì˜ˆì¸¡ í™•ë¥  ë°˜í™˜"""

        # ë² ì´ìŠ¤ ëª¨ë¸ ì˜ˆì¸¡
        base_predictions = np.zeros((len(X), len(self.models)))

        for i, (name, model) in enumerate(self.models.items()):
            base_predictions[:, i] = model.predict_proba(X)[:, 1]

        # ë©”íƒ€ ëª¨ë¸ ì˜ˆì¸¡
        final_predictions = self.meta_model.predict_proba(base_predictions)[:, 1]

        return np.column_stack([1 - final_predictions, final_predictions])

    def predict(self, X):
        """ì´ì§„ ì˜ˆì¸¡ ë°˜í™˜"""
        return (self.predict_proba(X)[:, 1] > 0.5).astype(int)

    def _clone_model(self, model):
        """ëª¨ë¸ ë³µì‚¬"""
        return type(model)(**model.get_params())

    def get_feature_importance(self, X):
        """íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°"""
        importance_dict = {}

        tree_models = ['xgb', 'lgb', 'cat', 'rf', 'et']
        for name in tree_models:
            if name in self.models:
                model = self.models[name]
                if hasattr(model, 'feature_importances_'):
                    importance_dict[name] = model.feature_importances_

        # í‰ê·  ì¤‘ìš”ë„ ê³„ì‚°
        if importance_dict:
            avg_importance = np.mean(list(importance_dict.values()), axis=0)
            return pd.DataFrame({
                'feature': X.columns,
                'importance': avg_importance
            }).sort_values('importance', ascending=False)

        return None

def hyperparameter_optimization(X, y, n_trials=50):
    """Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""

    def objective(trial):
        # XGBoost íŒŒë¼ë¯¸í„° ìµœì í™”
        params = {
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
            'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
        }

        model = xgb.XGBClassifier(
            objective='binary:logistic',
            eval_metric='auc',
            random_state=42,
            verbosity=0,
            **params
        )

        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
        scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)

        return scores.mean()

    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

    return study.best_params

def main_pipeline():
    """ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""

    print("ğŸ† Advanced Bank Churn Prediction Pipeline")
    print("="*60)

    # ë°ì´í„° ë¡œë”© (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Kaggle ë°ì´í„° ì‚¬ìš©)
    print("ğŸ“Š ë°ì´í„° ë¡œë”©...")
    # train = pd.read_csv('/kaggle/input/playground-series-s4e1/train.csv')
    # test = pd.read_csv('/kaggle/input/playground-series-s4e1/test.csv')

    # ë°ëª¨ìš© ë°ì´í„° ìƒì„± (ì‹¤ì œ ì‚¬ìš©ì‹œ ì£¼ì„ ì²˜ë¦¬)
    np.random.seed(42)
    train = generate_enhanced_sample_data(10000)
    test = generate_enhanced_sample_data(3000)
    test = test.drop('Exited', axis=1)

    print(f"Train shape: {train.shape}")
    print(f"Test shape: {test.shape}")

    # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
    print("\nğŸ”§ ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§...")
    fe_pipeline = AdvancedFeatureEngineering()

    # íƒ€ê²Ÿ ë¶„ë¦¬
    y = train['Exited']
    X = train.drop('Exited', axis=1)

    # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì ìš©
    fe_pipeline.fit(X)
    X_processed = fe_pipeline.transform(X)
    X_test_processed = fe_pipeline.transform(test)

    print(f"Original features: {X.shape[1]}")
    print(f"Engineered features: {X_processed.shape[1]}")

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ì˜µì…˜)
    optimize_params = False  # ì‹¤ì œ ì‚¬ìš©ì‹œ Trueë¡œ ë³€ê²½
    if optimize_params:
        print("\nâš¡ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”...")
        best_params = hyperparameter_optimization(X_processed, y, n_trials=20)
        print(f"Best parameters: {best_params}")

    # ê³ ê¸‰ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨
    print("\nğŸ­ ê³ ê¸‰ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨...")
    ensemble = AdvancedEnsemble(n_folds=5, random_state=42)
    ensemble.fit(X_processed, y)

    # êµì°¨ê²€ì¦ ì„±ëŠ¥ í‰ê°€
    print("\nğŸ“Š êµì°¨ê²€ì¦ ì„±ëŠ¥ í‰ê°€...")
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = []

    for fold, (train_idx, val_idx) in enumerate(cv.split(X_processed, y)):
        X_train_fold = X_processed.iloc[train_idx]
        y_train_fold = y.iloc[train_idx]
        X_val_fold = X_processed.iloc[val_idx]
        y_val_fold = y.iloc[val_idx]

        fold_ensemble = AdvancedEnsemble(n_folds=3, random_state=42)
        fold_ensemble.fit(X_train_fold, y_train_fold)

        val_pred = fold_ensemble.predict_proba(X_val_fold)[:, 1]
        score = roc_auc_score(y_val_fold, val_pred)
        cv_scores.append(score)

        print(f"Fold {fold + 1} ROC-AUC: {score:.4f}")

    print(f"\nCV ROC-AUC: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores) * 2:.4f})")

    # íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
    print("\nğŸ¯ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„...")
    feature_importance = ensemble.get_feature_importance(X_processed)
    if feature_importance is not None:
        print("ìƒìœ„ 15ê°œ ì¤‘ìš” íŠ¹ì„±:")
        print(feature_importance.head(15).to_string(index=False))

        # íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
        plt.figure(figsize=(12, 8))
        top_features = feature_importance.head(20)
        sns.barplot(data=top_features, x='importance', y='feature')
        plt.title('ğŸ¯ Top 20 Feature Importance (Ensemble Average)')
        plt.xlabel('Importance')
        plt.tight_layout()
        plt.show()

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
    print("\nğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡...")
    test_predictions = ensemble.predict_proba(X_test_processed)[:, 1]

    # ì œì¶œ íŒŒì¼ ìƒì„±
    submission = pd.DataFrame({
        'id': test['CustomerId'],
        'Exited': test_predictions
    })

    print(f"ì œì¶œ íŒŒì¼ ìƒì„±: {submission.shape}")
    print("ì˜ˆì¸¡ ë¶„í¬:")
    print(submission['Exited'].describe())

    # ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸
    print("\nğŸ’¼ ê³ ê¸‰ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸...")

    # í›ˆë ¨ ë°ì´í„° ì˜ˆì¸¡ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ ìƒì„±
    train_predictions = ensemble.predict_proba(X_processed)[:, 1]

    # ìœ„í—˜ë„ë³„ ì„¸ë¶„í™”
    risk_segments = pd.qcut(train_predictions, q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])
    segment_analysis = pd.DataFrame({
        'Risk_Segment': risk_segments,
        'Actual_Churn': y
    }).groupby('Risk_Segment')['Actual_Churn'].agg(['count', 'mean']).round(4)

    print("ìœ„í—˜ë„ë³„ ê³ ê° ì„¸ë¶„í™”:")
    print(segment_analysis)

    # ROI ê³„ì‚°
    high_risk_customers = len(train_predictions[train_predictions > 0.7])
    high_risk_churn_rate = y[train_predictions > 0.7].mean()

    print(f"\nğŸ’° ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ì¶”ì •:")
    print(f"â€¢ ê³ ìœ„í—˜ ê³ ê° ìˆ˜: {high_risk_customers:,}ëª…")
    print(f"â€¢ ê³ ìœ„í—˜êµ° ì‹¤ì œ ì´íƒˆë¥ : {high_risk_churn_rate:.1%}")
    print(f"â€¢ ì˜ˆìƒ ì´íƒˆ ë°©ì§€ íš¨ê³¼: {high_risk_churn_rate/y.mean():.1f}ë°° í–¥ìƒ")

    return ensemble, submission, feature_importance

def generate_enhanced_sample_data(n_samples=10000):
    """í–¥ìƒëœ ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œ íŒ¨í„´ ë°˜ì˜)"""
    np.random.seed(42)

    # ë” í˜„ì‹¤ì ì¸ ë°ì´í„° ìƒì„±
    data = {
        'CustomerId': range(15000000, 15000000 + n_samples),
        'Surname': [f'Customer_{i}' for i in range(n_samples)],
        'CreditScore': np.random.normal(650, 96, n_samples).astype(int),
        'Geography': np.random.choice(['France', 'Germany', 'Spain'], n_samples, p=[0.5, 0.25, 0.25]),
        'Gender': np.random.choice(['Male', 'Female'], n_samples, p=[0.545, 0.455]),
        'Age': np.random.normal(38.9, 10.5, n_samples),
        'Tenure': np.random.randint(0, 11, n_samples),
        'Balance': np.where(
            np.random.random(n_samples) < 0.16,  # 16% have zero balance
            0,
            np.random.lognormal(mean=10.5, sigma=1.2, size=n_samples)
        ),
        'NumOfProducts': np.random.choice([1, 2, 3, 4], n_samples, p=[0.509, 0.459, 0.030, 0.002]),
        'HasCrCard': np.random.choice([0, 1], n_samples, p=[0.294, 0.706]),
        'IsActiveMember': np.random.choice([0, 1], n_samples, p=[0.484, 0.516]),
        'EstimatedSalary': np.random.uniform(11.58, 199992.48, n_samples)
    }

    df = pd.DataFrame(data)

    # ë²”ìœ„ ì¡°ì •
    df['Age'] = np.clip(df['Age'], 18, 92).astype(int)
    df['CreditScore'] = np.clip(df['CreditScore'], 350, 850)
    df['Balance'] = np.clip(df['Balance'], 0, 250000)

    # í˜„ì‹¤ì ì¸ ì´íƒˆ í™•ë¥  ëª¨ë¸ë§
    # ë³µì¡í•œ ìƒí˜¸ì‘ìš© í¬í•¨
    age_effect = np.where(df['Age'] > 50, 0.3, -0.1)
    balance_effect = np.where(df['Balance'] == 0, 0.4, -0.1)
    products_effect = np.where(df['NumOfProducts'] == 1, 0.3,
                               np.where(df['NumOfProducts'] > 2, 0.2, -0.1))
    active_effect = np.where(df['IsActiveMember'] == 0, 0.25, -0.15)
    geo_effect = np.where(df['Geography'] == 'Germany', 0.15,
                          np.where(df['Geography'] == 'France', 0.0, 0.05))
    gender_effect = np.where(df['Gender'] == 'Female', 0.1, -0.05)
    credit_effect = np.where(df['CreditScore'] < 500, 0.2, -0.05)

    # ìƒí˜¸ì‘ìš© íš¨ê³¼
    age_gender_interaction = np.where((df['Age'] > 45) & (df['Gender'] == 'Female'), 0.1, 0)
    balance_products_interaction = np.where((df['Balance'] == 0) & (df['NumOfProducts'] == 1), 0.15, 0)

    churn_logit = (
            -1.5 +  # ê¸°ë³¸ ë¡œì§“
            age_effect +
            balance_effect +
            products_effect +
            active_effect +
            geo_effect +
            gender_effect +
            credit_effect +
            age_gender_interaction +
            balance_products_interaction +
            np.random.normal(0, 0.1, n_samples)  # ë…¸ì´ì¦ˆ
    )

    churn_prob = 1 / (1 + np.exp(-churn_logit))
    df['Exited'] = np.random.binomial(1, churn_prob, n_samples)

    return df

def advanced_model_interpretation(ensemble, X, y, feature_names):
    """ê³ ê¸‰ ëª¨ë¸ í•´ì„"""

    print("\nğŸ”¬ ê³ ê¸‰ ëª¨ë¸ í•´ì„ ë¶„ì„")
    print("="*40)

    # 1. ë¶€ë¶„ ì˜ì¡´ì„± ë¶„ì„ (ì‹œë®¬ë ˆì´ì…˜)
    print("ğŸ“ˆ ì£¼ìš” íŠ¹ì„±ë³„ ë¶€ë¶„ ì˜ì¡´ì„±:")

    important_features = ['Age', 'Balance_log1p', 'NumOfProducts', 'IsActiveMember', 'CreditScore']

    for feature in important_features:
        if feature in X.columns:
            feature_values = np.linspace(X[feature].min(), X[feature].max(), 20)
            partial_dependence = []

            X_temp = X.copy()
            for value in feature_values:
                X_temp[feature] = value
                pred = ensemble.predict_proba(X_temp)[:, 1].mean()
                partial_dependence.append(pred)

            plt.figure(figsize=(8, 5))
            plt.plot(feature_values, partial_dependence, 'b-', linewidth=2)
            plt.title(f'Partial Dependence: {feature}')
            plt.xlabel(feature)
            plt.ylabel('Predicted Churn Probability')
            plt.grid(True, alpha=0.3)
            plt.show()

    # 2. ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ ìœ„í—˜ í”„ë¡œíŒŒì¼
    predictions = ensemble.predict_proba(X)[:, 1]

    segments = {
        'Young_Active': (X['Age'] <= 35) & (X['IsActiveMember'] == 1),
        'Middle_Inactive': (X['Age'].between(36, 50)) & (X['IsActiveMember'] == 0),
        'Senior_Multiple': (X['Age'] > 50) & (X['NumOfProducts'] > 1),
        'Zero_Balance': X['Balance'] == 0,
        'High_Value': X['Balance'] > X['Balance'].quantile(0.8)
    }

    print("\nğŸ‘¥ ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ ìœ„í—˜ ë¶„ì„:")
    for segment_name, mask in segments.items():
        if mask.sum() > 0:
            avg_risk = predictions[mask].mean()
            actual_churn = y[mask].mean()
            count = mask.sum()
            print(f"â€¢ {segment_name}: ì˜ˆì¸¡ìœ„í—˜ {avg_risk:.3f}, ì‹¤ì œì´íƒˆ {actual_churn:.3f}, ê³ ê°ìˆ˜ {count:,}")

def create_business_dashboard_data(ensemble, X, y, test_predictions):
    """ë¹„ì¦ˆë‹ˆìŠ¤ ëŒ€ì‹œë³´ë“œìš© ë°ì´í„° ìƒì„±"""

    train_predictions = ensemble.predict_proba(X)[:, 1]

    dashboard_data = {
        'model_performance': {
            'train_auc': roc_auc_score(y, train_predictions),
            'train_accuracy': ((train_predictions > 0.5) == y).mean(),
            'precision_at_10_percent': y[train_predictions > np.percentile(train_predictions, 90)].mean()
        },

        'risk_distribution': {
            'very_high_risk': (train_predictions > 0.8).sum(),
            'high_risk': ((train_predictions > 0.6) & (train_predictions <= 0.8)).sum(),
            'medium_risk': ((train_predictions > 0.4) & (train_predictions <= 0.6)).sum(),
            'low_risk': (train_predictions <= 0.4).sum()
        },

        'business_metrics': {
            'total_customers': len(X),
            'predicted_churners': (train_predictions > 0.5).sum(),
            'high_value_at_risk': ((X['Balance'] > X['Balance'].quantile(0.75)) &
                                   (train_predictions > 0.6)).sum(),
            'campaign_target_size': (train_predictions > 0.7).sum()
        }
    }

    return dashboard_data

# ì‹¤í–‰ë¶€
if __name__ == "__main__":

    print("ğŸš€ Starting Advanced Bank Churn Prediction Pipeline...")

    # ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    ensemble, submission, feature_importance = main_pipeline()

    # ê³ ê¸‰ í•´ì„ ë¶„ì„ (ì„ íƒì‚¬í•­)
    run_interpretation = True
    if run_interpretation:
        # ë°ëª¨ ë°ì´í„°ë¡œ í•´ì„ ë¶„ì„
        demo_train = generate_enhanced_sample_data(5000)
        demo_y = demo_train['Exited']
        demo_X = demo_train.drop('Exited', axis=1)

        fe_pipeline = AdvancedFeatureEngineering()
        fe_pipeline.fit(demo_X)
        demo_X_processed = fe_pipeline.transform(demo_X)

        advanced_model_interpretation(ensemble, demo_X_processed, demo_y,
                                      fe_pipeline.feature_names)

    # ë¹„ì¦ˆë‹ˆìŠ¤ ëŒ€ì‹œë³´ë“œ ë°ì´í„°
    dashboard_data = create_business_dashboard_data(
        ensemble,
        demo_X_processed if 'demo_X_processed' in locals() else None,
        demo_y if 'demo_y' in locals() else None,
        submission['Exited'].values
    )

    print("\nğŸ“Š ë¹„ì¦ˆë‹ˆìŠ¤ ëŒ€ì‹œë³´ë“œ ë©”íŠ¸ë¦­:")
    for category, metrics in dashboard_data.items():
        print(f"\n{category.upper()}:")
        for metric, value in metrics.items():
            if isinstance(value, float):
                print(f"  â€¢ {metric}: {value:.4f}")
            else:
                print(f"  â€¢ {metric}: {value:,}")

    # ìµœì¢… ì œì¶œ íŒŒì¼ ì €ì¥
    submission.to_csv('advanced_ensemble_submission.csv', index=False)

    print(f"\nâœ… Advanced Ensemble Pipeline ì™„ë£Œ!")
    print(f"ğŸ“ ì œì¶œ íŒŒì¼: advanced_ensemble_submission.csv")
    print(f"ğŸ¯ ì˜ˆìƒ ì„±ëŠ¥: ROC-AUC > 0.85")
    print(f"ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜: ê³ ìœ„í—˜ ê³ ê° ì‹ë³„ ì •í™•ë„ í–¥ìƒ")

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    import gc
    gc.collect()

    print("ğŸ‰ All processes completed successfully!")