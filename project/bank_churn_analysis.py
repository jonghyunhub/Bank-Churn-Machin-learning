#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¦ Bank Customer Churn Prediction - Kaggle Playground Series S4E1

ëª©í‘œ: ì€í–‰ ê³ ê°ì˜ ì´íƒˆ(churn) í™•ë¥  ì˜ˆì¸¡
ë°ì´í„°: Kaggle Playground Series S4E1 (í•©ì„± ë°ì´í„°)
í‰ê°€ì§€í‘œ: ROC AUC
ì ‘ê·¼ë²•: EDA â†’ Feature Engineering â†’ Ensemble Modeling

Author: Data Science Team
Date: 2025-06-07
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
import xgboost as xgb
import lightgbm as lgb
import gc

# ì„¤ì •
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)

def print_header(title):
    """í—¤ë” ì¶œë ¥ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print(f"ğŸ¦ {title}")
    print("="*60)

def print_section(title):
    """ì„¹ì…˜ ì¶œë ¥ í•¨ìˆ˜"""
    print(f"\nğŸ“Š {title}")
    print("-"*40)

def load_data():
    """ë°ì´í„° ë¡œë”© ë° ê¸°ë³¸ í™•ì¸"""
    print_header("Bank Customer Churn Prediction - Playground Series S4E1")

    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ í™•ì¸
    data_files = {
        'train': '../data/train.csv',
        'test': '../data/test.csv',
        'sample_submission': '../data/sample_submission.csv'
    }

    print_section("ë°ì´í„° íŒŒì¼ í™•ì¸")
    for name, path in data_files.items():
        if os.path.exists(path):
            print(f"âœ… {name}: {path}")
        else:
            print(f"âŒ {name} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")

    # ì‹¤ì œ ë°ì´í„° ë¡œë”©
    print_section("ë°ì´í„° ë¡œë”©")
    try:
        train = pd.read_csv('../data/train.csv')
        test = pd.read_csv('../data/test.csv')
        sample_submission = pd.read_csv('../data/sample_submission.csv')

        print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ!")
        print(f"Train shape: {train.shape}")
        print(f"Test shape: {test.shape}")
        print(f"Sample submission shape: {sample_submission.shape}")

        return train, test, sample_submission

    except FileNotFoundError as e:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return None, None, None

def basic_data_analysis(train, test):
    """ê¸°ë³¸ ë°ì´í„° ë¶„ì„"""
    print_section("ê¸°ë³¸ ë°ì´í„° ì •ë³´")

    print("ğŸ” Train ë°ì´í„° ê¸°ë³¸ ì •ë³´:")
    print(train.info())

    print("\nğŸ“Š Train ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 5í–‰):")
    print(train.head())

    print("\nğŸ“Š Test ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 5í–‰):")
    print(test.head())

    # íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬ í™•ì¸
    print(f"\nğŸ“ˆ íƒ€ê²Ÿ ë³€ìˆ˜ (Exited) ë¶„í¬:")
    target_dist = train['Exited'].value_counts(normalize=True)
    print(target_dist)

    churn_rate = train['Exited'].mean()
    print(f"\nì „ì²´ ì´íƒˆë¥ : {churn_rate:.2%}")

    # ê²°ì¸¡ì¹˜ í™•ì¸
    print(f"\nâŒ Train ë°ì´í„° ê²°ì¸¡ì¹˜:")
    missing_train = train.isnull().sum()
    if missing_train.sum() == 0:
        print("ê²°ì¸¡ì¹˜ ì—†ìŒ")
    else:
        print(missing_train[missing_train > 0])

    print(f"\nâŒ Test ë°ì´í„° ê²°ì¸¡ì¹˜:")
    missing_test = test.isnull().sum()
    if missing_test.sum() == 0:
        print("ê²°ì¸¡ì¹˜ ì—†ìŒ")
    else:
        print(missing_test[missing_test > 0])

    # ê¸°ìˆ  í†µê³„
    print("\nğŸ“Š ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê¸°ìˆ  í†µê³„:")
    print(train.describe())

    print("\nğŸ“Š ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬:")
    categorical_cols = ['Geography', 'Gender']
    for col in categorical_cols:
        print(f"\n{col} ë¶„í¬:")
        print(train[col].value_counts())

    return churn_rate

def create_comprehensive_eda(df, save_plots=True):
    """ì¢…í•©ì ì¸ EDA ì‹œê°í™”"""
    print_section("íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)")

    fig, axes = plt.subplots(4, 3, figsize=(20, 24))
    fig.suptitle('ğŸ¦ Bank Customer Churn - Comprehensive EDA', fontsize=16, y=0.98)

    # 1. ì´íƒˆë¥  by ì§€ì—­
    churn_by_geo = df.groupby('Geography')['Exited'].agg(['count', 'mean']).reset_index()
    axes[0,0].bar(churn_by_geo['Geography'], churn_by_geo['mean'], alpha=0.7, color='skyblue')
    axes[0,0].set_title('ì´íƒˆë¥  by ì§€ì—­')
    axes[0,0].set_ylabel('Churn Rate')
    for i, v in enumerate(churn_by_geo['mean']):
        axes[0,0].text(i, v + 0.01, f'{v:.2%}', ha='center', va='bottom')

    # 2. ì´íƒˆë¥  by ì„±ë³„
    churn_by_gender = df.groupby('Gender')['Exited'].mean()
    axes[0,1].bar(churn_by_gender.index, churn_by_gender.values, alpha=0.7, color='lightcoral')
    axes[0,1].set_title('ì´íƒˆë¥  by ì„±ë³„')
    axes[0,1].set_ylabel('Churn Rate')
    for i, v in enumerate(churn_by_gender.values):
        axes[0,1].text(i, v + 0.01, f'{v:.2%}', ha='center', va='bottom')

    # 3. ë‚˜ì´ ë¶„í¬
    axes[0,2].hist(df[df['Exited']==0]['Age'].dropna(), alpha=0.6, label='Stayed', bins=30, color='green')
    axes[0,2].hist(df[df['Exited']==1]['Age'].dropna(), alpha=0.6, label='Churned', bins=30, color='red')
    axes[0,2].set_title('ë‚˜ì´ ë¶„í¬')
    axes[0,2].set_xlabel('Age')
    axes[0,2].legend()

    # 4. ì‹ ìš©ì ìˆ˜ ë¶„í¬
    axes[1,0].hist(df[df['Exited']==0]['CreditScore'], alpha=0.6, label='Stayed', bins=30, color='green')
    axes[1,0].hist(df[df['Exited']==1]['CreditScore'], alpha=0.6, label='Churned', bins=30, color='red')
    axes[1,0].set_title('ì‹ ìš©ì ìˆ˜ ë¶„í¬')
    axes[1,0].set_xlabel('Credit Score')
    axes[1,0].legend()

    # 5. ì”ì•¡ ë¶„í¬ (0 ì œì™¸)
    stayed_balance = df[df['Exited']==0]['Balance']
    churned_balance = df[df['Exited']==1]['Balance']
    axes[1,1].hist(stayed_balance[stayed_balance > 0], alpha=0.6, label='Stayed', bins=50, color='green')
    axes[1,1].hist(churned_balance[churned_balance > 0], alpha=0.6, label='Churned', bins=50, color='red')
    axes[1,1].set_title('ì”ì•¡ ë¶„í¬ (Balance > 0)')
    axes[1,1].set_xlabel('Balance')
    axes[1,1].set_xscale('log')
    axes[1,1].legend()

    # 6. ì œí’ˆ ìˆ˜ë³„ ì´íƒˆë¥ 
    product_churn = df.groupby('NumOfProducts')['Exited'].agg(['count', 'mean']).reset_index()
    bars = axes[1,2].bar(product_churn['NumOfProducts'], product_churn['mean'], alpha=0.7, color='orange')
    axes[1,2].set_title('ì œí’ˆ ìˆ˜ë³„ ì´íƒˆë¥ ')
    axes[1,2].set_xlabel('Number of Products')
    axes[1,2].set_ylabel('Churn Rate')
    for bar, rate in zip(bars, product_churn['mean']):
        axes[1,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{rate:.2%}', ha='center', va='bottom')

    # 7. í™œì„± íšŒì› ì—¬ë¶€ë³„ ì´íƒˆë¥ 
    active_churn = df.groupby('IsActiveMember')['Exited'].mean()
    bars = axes[2,0].bar(['Inactive', 'Active'], active_churn.values, alpha=0.7, color='purple')
    axes[2,0].set_title('í™œì„± íšŒì› ì—¬ë¶€ë³„ ì´íƒˆë¥ ')
    axes[2,0].set_ylabel('Churn Rate')
    for bar, rate in zip(bars, active_churn.values):
        axes[2,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{rate:.2%}', ha='center', va='bottom')

    # 8. ì‹ ìš©ì¹´ë“œ ë³´ìœ  ì—¬ë¶€ë³„ ì´íƒˆë¥ 
    card_churn = df.groupby('HasCrCard')['Exited'].mean()
    bars = axes[2,1].bar(['No Card', 'Has Card'], card_churn.values, alpha=0.7, color='brown')
    axes[2,1].set_title('ì‹ ìš©ì¹´ë“œ ë³´ìœ  ì—¬ë¶€ë³„ ì´íƒˆë¥ ')
    axes[2,1].set_ylabel('Churn Rate')
    for bar, rate in zip(bars, card_churn.values):
        axes[2,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{rate:.2%}', ha='center', va='bottom')

    # 9. ì¬ì§ê¸°ê°„ë³„ ì´íƒˆë¥ 
    tenure_churn = df.groupby('Tenure')['Exited'].mean()
    axes[2,2].plot(tenure_churn.index, tenure_churn.values, marker='o', linewidth=2, markersize=6)
    axes[2,2].set_title('ì¬ì§ê¸°ê°„ë³„ ì´íƒˆë¥ ')
    axes[2,2].set_xlabel('Tenure (years)')
    axes[2,2].set_ylabel('Churn Rate')
    axes[2,2].grid(True, alpha=0.3)

    # 10. ë‚˜ì´ëŒ€ë³„ ìƒì„¸ ë¶„ì„
    age_bins = [0, 25, 35, 45, 55, 65, 100]
    age_labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '65+']
    df_temp = df.copy()
    df_temp['AgeGroup'] = pd.cut(df_temp['Age'], bins=age_bins, labels=age_labels)
    age_group_churn = df_temp.groupby('AgeGroup')['Exited'].mean()
    bars = axes[3,0].bar(range(len(age_group_churn)), age_group_churn.values, alpha=0.7, color='teal')
    axes[3,0].set_title('ë‚˜ì´ëŒ€ë³„ ì´íƒˆë¥ ')
    axes[3,0].set_xticks(range(len(age_group_churn)))
    axes[3,0].set_xticklabels(age_group_churn.index, rotation=45)
    axes[3,0].set_ylabel('Churn Rate')
    for bar, rate in zip(bars, age_group_churn.values):
        axes[3,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{rate:.2%}', ha='center', va='bottom')

    # 11. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    numeric_cols = numeric_cols.drop(['id', 'CustomerId'])  # ID ì»¬ëŸ¼ ì œì™¸
    corr_matrix = df[numeric_cols].corr()
    sns.heatmap(corr_matrix, annot=True, cmap='RdYlBu_r', center=0,
                ax=axes[3,1], cbar_kws={'shrink': 0.8}, fmt='.2f')
    axes[3,1].set_title('ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„')

    # 12. ì”ì•¡ 0 vs ì´íƒˆë¥ 
    balance_zero = df['Balance'] == 0
    balance_zero_churn = df.groupby(balance_zero)['Exited'].mean()
    bars = axes[3,2].bar(['Balance > 0', 'Balance = 0'], balance_zero_churn.values, alpha=0.7, color='gold')
    axes[3,2].set_title('ì”ì•¡ ìœ ë¬´ë³„ ì´íƒˆë¥ ')
    axes[3,2].set_ylabel('Churn Rate')
    for bar, rate in zip(bars, balance_zero_churn.values):
        axes[3,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{rate:.2%}', ha='center', va='bottom')

    plt.tight_layout()

    if save_plots:
        plt.savefig('eda_analysis.png', dpi=300, bbox_inches='tight')
        print("ğŸ“Š EDA ì°¨íŠ¸ ì €ì¥ë¨: eda_analysis.png")

    plt.show()

class AdvancedFeatureEngineering:
    """ê³ ë„í™”ëœ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ í´ë˜ìŠ¤"""

    def __init__(self):
        self.label_encoders = {}
        self.fitted = False

    def fit(self, df):
        """í›ˆë ¨ ë°ì´í„°ë¡œ ì¸ì½”ë” í•™ìŠµ"""
        # ë²”ì£¼í˜• ë³€ìˆ˜ ë¼ë²¨ ì¸ì½”ë”© í•™ìŠµ
        categorical_cols = ['Geography', 'Gender']
        for col in categorical_cols:
            if col in df.columns:
                le = LabelEncoder()
                le.fit(df[col].dropna())
                self.label_encoders[col] = le

        self.fitted = True
        return self

    def transform(self, df):
        """íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì ìš©"""
        if not self.fitted:
            raise ValueError("ë¨¼ì € fit() ë©”ì„œë“œë¥¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.")

        df = df.copy()

        # 1. ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
        for col, le in self.label_encoders.items():
            if col in df.columns:
                df[f'{col}_encoded'] = le.transform(df[col])

        # 2. ë‚˜ì´ ê´€ë ¨ íŠ¹ì„±
        df['Age_squared'] = df['Age'] ** 2
        df['Age_log'] = np.log1p(df['Age'])
        df['is_senior'] = (df['Age'] >= 50).astype(int)
        df['is_young'] = (df['Age'] <= 30).astype(int)
        df['is_middle_aged'] = ((df['Age'] > 30) & (df['Age'] < 50)).astype(int)

        # ë‚˜ì´ ê·¸ë£¹ (ì•ˆì „í•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬)
        try:
            age_bins = [0, 30, 40, 50, 60, 100]
            df['age_group'] = pd.cut(df['Age'], bins=age_bins, labels=[0, 1, 2, 3, 4])
            df['age_group'] = df['age_group'].astype(float)
        except ValueError:
            # ë‚˜ì´ ë°ì´í„°ì— ë¬¸ì œê°€ ìˆëŠ” ê²½ìš° ì•ˆì „í•œ ì²˜ë¦¬
            df['age_group'] = 0
            df.loc[df['Age'] <= 30, 'age_group'] = 0
            df.loc[(df['Age'] > 30) & (df['Age'] <= 40), 'age_group'] = 1
            df.loc[(df['Age'] > 40) & (df['Age'] <= 50), 'age_group'] = 2
            df.loc[(df['Age'] > 50) & (df['Age'] <= 60), 'age_group'] = 3
            df.loc[df['Age'] > 60, 'age_group'] = 4

        # 3. ì‹ ìš©ì ìˆ˜ ê´€ë ¨ íŠ¹ì„±
        df['CreditScore_normalized'] = df['CreditScore'] / 850
        df['CreditScore_squared'] = df['CreditScore'] ** 2
        df['is_excellent_credit'] = (df['CreditScore'] >= 750).astype(int)
        df['is_poor_credit'] = (df['CreditScore'] <= 580).astype(int)

        # ì‹ ìš©ì ìˆ˜ ë“±ê¸‰ (ì•ˆì „í•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬)
        try:
            credit_bins = [0, 580, 670, 740, 800, 850]
            df['credit_grade'] = pd.cut(df['CreditScore'], bins=credit_bins, labels=[0, 1, 2, 3, 4])
            df['credit_grade'] = df['credit_grade'].astype(float)
        except ValueError:
            # ì‹ ìš©ì ìˆ˜ ë°ì´í„°ì— ë¬¸ì œê°€ ìˆëŠ” ê²½ìš° ì•ˆì „í•œ ì²˜ë¦¬
            df['credit_grade'] = 0
            df.loc[df['CreditScore'] <= 580, 'credit_grade'] = 0
            df.loc[(df['CreditScore'] > 580) & (df['CreditScore'] <= 670), 'credit_grade'] = 1
            df.loc[(df['CreditScore'] > 670) & (df['CreditScore'] <= 740), 'credit_grade'] = 2
            df.loc[(df['CreditScore'] > 740) & (df['CreditScore'] <= 800), 'credit_grade'] = 3
            df.loc[df['CreditScore'] > 800, 'credit_grade'] = 4

        # 4. ì”ì•¡ ê´€ë ¨ íŠ¹ì„±
        df['has_balance'] = (df['Balance'] > 0).astype(int)
        df['Balance_log1p'] = np.log1p(df['Balance'])
        df['Balance_sqrt'] = np.sqrt(np.maximum(df['Balance'], 0))  # ìŒìˆ˜ê°’ ë°©ì§€
        df['is_zero_balance'] = (df['Balance'] == 0).astype(int)
        df['Balance_to_Salary_ratio'] = df['Balance'] / (df['EstimatedSalary'] + 1e-8)

        # ì”ì•¡ êµ¬ê°„ (robustí•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬)
        try:
            df['balance_quartile'] = pd.qcut(df['Balance'], q=4, labels=[0, 1, 2, 3], duplicates='drop')
            df['balance_quartile'] = df['balance_quartile'].astype(float)
        except (ValueError, TypeError):
            # qcutì´ ì‹¤íŒ¨í•˜ë©´ ìˆ˜ë™ìœ¼ë¡œ êµ¬ê°„ ìƒì„± (ì¤‘ë³µ ì œê±°)
            balance_values = df['Balance']
            q25 = balance_values.quantile(0.25)
            q50 = balance_values.quantile(0.5)
            q75 = balance_values.quantile(0.75)

            # ì¤‘ë³µëœ ê°’ë“¤ì„ ì œê±°í•˜ê³  uniqueí•œ êµ¬ê°„ ìƒì„±
            unique_thresholds = []
            for threshold in [q25, q50, q75]:
                if threshold not in unique_thresholds:
                    unique_thresholds.append(threshold)

            # êµ¬ê°„ë³„ í• ë‹¹
            if len(unique_thresholds) >= 3:
                df['balance_quartile'] = 0
                df.loc[balance_values > unique_thresholds[0], 'balance_quartile'] = 1
                df.loc[balance_values > unique_thresholds[1], 'balance_quartile'] = 2
                df.loc[balance_values > unique_thresholds[2], 'balance_quartile'] = 3
            elif len(unique_thresholds) >= 1:
                df['balance_quartile'] = 0
                df.loc[balance_values > unique_thresholds[0], 'balance_quartile'] = 1
                if len(unique_thresholds) >= 2:
                    df.loc[balance_values > unique_thresholds[1], 'balance_quartile'] = 2
            else:
                # ëª¨ë“  ê°’ì´ ê°™ì€ ê²½ìš°
                df['balance_quartile'] = 0

        # 5. ì œí’ˆ ê´€ë ¨ íŠ¹ì„±
        df['single_product'] = (df['NumOfProducts'] == 1).astype(int)
        df['multiple_products'] = (df['NumOfProducts'] > 2).astype(int)
        df['products_per_tenure'] = df['NumOfProducts'] / (df['Tenure'] + 1)
        df['active_products'] = df['IsActiveMember'] * df['NumOfProducts']

        # 6. ê¸‰ì—¬ ê´€ë ¨ íŠ¹ì„±
        df['EstimatedSalary_log'] = np.log1p(df['EstimatedSalary'])
        df['salary_per_age'] = df['EstimatedSalary'] / (df['Age'] + 1e-8)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€

        # ê¸‰ì—¬ êµ¬ê°„ (robustí•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬)
        try:
            df['salary_quartile'] = pd.qcut(df['EstimatedSalary'], q=4, labels=[0, 1, 2, 3], duplicates='drop')
            df['salary_quartile'] = df['salary_quartile'].astype(float)
        except (ValueError, TypeError):
            # qcutì´ ì‹¤íŒ¨í•˜ë©´ ìˆ˜ë™ìœ¼ë¡œ êµ¬ê°„ ìƒì„±
            salary_values = df['EstimatedSalary']
            s_q25 = salary_values.quantile(0.25)
            s_q50 = salary_values.quantile(0.5)
            s_q75 = salary_values.quantile(0.75)

            df['salary_quartile'] = 0
            df.loc[salary_values > s_q25, 'salary_quartile'] = 1
            df.loc[salary_values > s_q50, 'salary_quartile'] = 2
            df.loc[salary_values > s_q75, 'salary_quartile'] = 3

        # 7. ì¢…í•© ì ìˆ˜ë“¤
        df['wealth_score'] = (
                df['CreditScore_normalized'] * 0.3 +
                (df['Balance_log1p'] / df['Balance_log1p'].max()) * 0.4 +
                (df['EstimatedSalary'] / df['EstimatedSalary'].max()) * 0.3
        )

        df['engagement_score'] = (
                df['IsActiveMember'] * 0.4 +
                df['HasCrCard'] * 0.2 +
                (df['NumOfProducts'] / 4) * 0.4
        )

        df['risk_score'] = (
                df['is_senior'] * 0.3 +
                df['is_zero_balance'] * 0.3 +
                df['single_product'] * 0.2 +
                (1 - df['IsActiveMember']) * 0.2
        )

        # 8. ìƒí˜¸ì‘ìš© íŠ¹ì„± (ì•ˆì „í•œ ê³„ì‚°)
        df['age_balance_interaction'] = df['Age'] * df['Balance_log1p']
        df['credit_salary_interaction'] = df['CreditScore'] * df['EstimatedSalary_log']
        df['products_tenure_interaction'] = df['NumOfProducts'] * (df['Tenure'] + 1)  # 0 ë°©ì§€
        df['active_balance_interaction'] = df['IsActiveMember'] * df['Balance_log1p']

        # 9. ì§€ì—­/ì„±ë³„ ì¡°í•©
        if 'Geography_encoded' in df.columns and 'Gender_encoded' in df.columns:
            df['geo_gender_combo'] = df['Geography_encoded'] * 10 + df['Gender_encoded']

        return df

    def fit_transform(self, df):
        """fitê³¼ transformì„ í•œë²ˆì— ìˆ˜í–‰"""
        return self.fit(df).transform(df)

def feature_engineering_pipeline(train, test):
    """íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ íŒŒì´í”„ë¼ì¸"""
    print_section("íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§")

    print("ğŸ”§ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ìˆ˜í–‰ ì¤‘...")
    fe_pipeline = AdvancedFeatureEngineering()

    # í›ˆë ¨ ë°ì´í„°ë¡œ í•™ìŠµ í›„ ë³€í™˜
    train_fe = fe_pipeline.fit_transform(train)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜
    test_fe = fe_pipeline.transform(test)

    print(f"âœ… íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ!")
    print(f"Original features: {train.shape[1]}")
    print(f"Engineered features: {train_fe.shape[1]}")
    print(f"Added features: {train_fe.shape[1] - train.shape[1]}")

    return train_fe, test_fe

def prepare_modeling_data(df, target_col=None):
    """ëª¨ë¸ë§ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
    # ì œê±°í•  ì»¬ëŸ¼ë“¤
    drop_cols = ['id', 'CustomerId', 'Surname', 'Geography', 'Gender']

    # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì œê±°
    cols_to_drop = [col for col in drop_cols if col in df.columns]
    if target_col and target_col in df.columns:
        cols_to_drop.append(target_col)

    features = df.drop(columns=cols_to_drop)

    if target_col and target_col in df.columns:
        target = df[target_col]
        return features, target
    else:
        return features

def train_models(X, y):
    """ëª¨ë¸ í›ˆë ¨ ë° êµì°¨ê²€ì¦"""
    print_section("ëª¨ë¸ í›ˆë ¨ ë° êµì°¨ê²€ì¦")

    # êµì°¨ê²€ì¦ ì„¤ì •
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    print("ğŸš€ ëª¨ë¸ í›ˆë ¨ ë° êµì°¨ê²€ì¦ ì‹œì‘...")

    # 1. XGBoost ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸
    print("\nğŸ“Š XGBoost ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í›ˆë ¨...")

    try:
        xgb_params = {
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'max_depth': 6,
            'learning_rate': 0.1,
            'n_estimators': 300,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'reg_alpha': 1,
            'reg_lambda': 1,
            'random_state': 42,
            'verbosity': 0,
            'use_label_encoder': False  # ê²½ê³  ë°©ì§€
        }

        xgb_model = xgb.XGBClassifier(**xgb_params)

        # ìˆ˜ë™ êµì°¨ê²€ì¦ (í˜¸í™˜ì„± ë¬¸ì œ ë°©ì§€)
        xgb_cv_scores = []
        for train_idx, val_idx in cv.split(X, y):
            X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

            xgb_model_fold = xgb.XGBClassifier(**xgb_params)
            xgb_model_fold.fit(X_train_fold, y_train_fold)
            y_pred_proba = xgb_model_fold.predict_proba(X_val_fold)[:, 1]
            score = roc_auc_score(y_val_fold, y_pred_proba)
            xgb_cv_scores.append(score)

        xgb_cv_scores = np.array(xgb_cv_scores)
        print(f"XGBoost CV ROC-AUC: {xgb_cv_scores.mean():.4f} (+/- {xgb_cv_scores.std() * 2:.4f})")

        # ì „ì²´ ë°ì´í„°ë¡œ ëª¨ë¸ í›ˆë ¨
        xgb_model.fit(X, y)

    except Exception as e:
        print(f"XGBoost ì˜¤ë¥˜: {e}")
        print("XGBoostë¥¼ ê±´ë„ˆë›°ê³  ë‹¤ë¥¸ ëª¨ë¸ë“¤ë¡œ ê³„ì† ì§„í–‰...")
        xgb_model = None
        xgb_cv_scores = np.array([0.5])

    # 2. LightGBM ëª¨ë¸
    print("\nâš¡ LightGBM ëª¨ë¸ í›ˆë ¨...")

    try:
        lgb_params = {
            'objective': 'binary',
            'metric': 'auc',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.1,
            'n_estimators': 300,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'reg_alpha': 1,
            'reg_lambda': 1,
            'random_state': 42,
            'verbosity': -1
        }

        lgb_model = lgb.LGBMClassifier(**lgb_params)

        # ìˆ˜ë™ êµì°¨ê²€ì¦
        lgb_cv_scores = []
        for train_idx, val_idx in cv.split(X, y):
            X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

            lgb_model_fold = lgb.LGBMClassifier(**lgb_params)
            lgb_model_fold.fit(X_train_fold, y_train_fold)
            y_pred_proba = lgb_model_fold.predict_proba(X_val_fold)[:, 1]
            score = roc_auc_score(y_val_fold, y_pred_proba)
            lgb_cv_scores.append(score)

        lgb_cv_scores = np.array(lgb_cv_scores)
        print(f"LightGBM CV ROC-AUC: {lgb_cv_scores.mean():.4f} (+/- {lgb_cv_scores.std() * 2:.4f})")

        lgb_model.fit(X, y)

    except Exception as e:
        print(f"LightGBM ì˜¤ë¥˜: {e}")
        print("LightGBMì„ ê±´ë„ˆë›°ê³  ë‹¤ë¥¸ ëª¨ë¸ë“¤ë¡œ ê³„ì† ì§„í–‰...")
        lgb_model = None
        lgb_cv_scores = np.array([0.5])

    # 3. Random Forest ëª¨ë¸
    print("\nğŸŒ² Random Forest ëª¨ë¸ í›ˆë ¨...")

    try:
        rf_model = RandomForestClassifier(
            n_estimators=200,
            max_depth=12,
            min_samples_split=5,
            min_samples_leaf=2,
            max_features='sqrt',
            random_state=42,
            n_jobs=-1
        )

        # ìˆ˜ë™ êµì°¨ê²€ì¦
        rf_cv_scores = []
        for train_idx, val_idx in cv.split(X, y):
            X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

            rf_model_fold = RandomForestClassifier(
                n_estimators=200,
                max_depth=12,
                min_samples_split=5,
                min_samples_leaf=2,
                max_features='sqrt',
                random_state=42,
                n_jobs=-1
            )
            rf_model_fold.fit(X_train_fold, y_train_fold)
            y_pred_proba = rf_model_fold.predict_proba(X_val_fold)[:, 1]
            score = roc_auc_score(y_val_fold, y_pred_proba)
            rf_cv_scores.append(score)

        rf_cv_scores = np.array(rf_cv_scores)
        print(f"Random Forest CV ROC-AUC: {rf_cv_scores.mean():.4f} (+/- {rf_cv_scores.std() * 2:.4f})")

        rf_model.fit(X, y)

    except Exception as e:
        print(f"Random Forest ì˜¤ë¥˜: {e}")
        print("Random Forestë¥¼ ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰...")
        rf_model = None
        rf_cv_scores = np.array([0.5])

    # 4. Logistic Regression (ë°±ì—… ëª¨ë¸)
    print("\nğŸ“ˆ Logistic Regression ëª¨ë¸ í›ˆë ¨...")

    try:
        from sklearn.preprocessing import StandardScaler

        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)

        lr_model = LogisticRegression(
            max_iter=1000,
            random_state=42,
            C=1.0
        )

        # ìˆ˜ë™ êµì°¨ê²€ì¦
        lr_cv_scores = []
        for train_idx, val_idx in cv.split(X_scaled, y):
            X_train_fold, X_val_fold = X_scaled.iloc[train_idx], X_scaled.iloc[val_idx]
            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

            lr_model_fold = LogisticRegression(max_iter=1000, random_state=42, C=1.0)
            lr_model_fold.fit(X_train_fold, y_train_fold)
            y_pred_proba = lr_model_fold.predict_proba(X_val_fold)[:, 1]
            score = roc_auc_score(y_val_fold, y_pred_proba)
            lr_cv_scores.append(score)

        lr_cv_scores = np.array(lr_cv_scores)
        print(f"Logistic Regression CV ROC-AUC: {lr_cv_scores.mean():.4f} (+/- {lr_cv_scores.std() * 2:.4f})")

        lr_model.fit(X_scaled, y)

    except Exception as e:
        print(f"Logistic Regression ì˜¤ë¥˜: {e}")
        lr_model = None
        lr_cv_scores = np.array([0.5])
        scaler = None

    # ê²°ê³¼ ì •ë¦¬
    models = {
        'xgb': xgb_model,
        'lgb': lgb_model,
        'rf': rf_model,
        'lr': lr_model
    }

    cv_scores = {
        'xgb': xgb_cv_scores,
        'lgb': lgb_cv_scores,
        'rf': rf_cv_scores,
        'lr': lr_cv_scores
    }

    # scalerë„ í•¨ê»˜ ë°˜í™˜ (LRì—ì„œ í•„ìš”)
    if 'scaler' in locals():
        models['scaler'] = scaler

    return models, cv_scores

def create_ensemble_and_predict(models, cv_scores, X, X_test, y):
    """ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ë° ì˜ˆì¸¡"""
    print_section("ì•™ìƒë¸” ëª¨ë¸ë§")

    # ì„±ê³µì ìœ¼ë¡œ í›ˆë ¨ëœ ëª¨ë¸ë“¤ë§Œ ì‚¬ìš©
    valid_models = {k: v for k, v in models.items() if v is not None and k != 'scaler'}

    if len(valid_models) == 0:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
        return None, None, None

    print(f"ğŸ­ ì•™ìƒë¸” ëª¨ë¸ ìƒì„±... (ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(valid_models.keys())})")

    # ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ (í›ˆë ¨ ë°ì´í„°)
    train_predictions = {}
    test_predictions = {}

    # XGBoost
    if models['xgb'] is not None:
        train_predictions['xgb'] = models['xgb'].predict_proba(X)[:, 1]
        test_predictions['xgb'] = models['xgb'].predict_proba(X_test)[:, 1]

    # LightGBM
    if models['lgb'] is not None:
        train_predictions['lgb'] = models['lgb'].predict_proba(X)[:, 1]
        test_predictions['lgb'] = models['lgb'].predict_proba(X_test)[:, 1]

    # Random Forest
    if models['rf'] is not None:
        train_predictions['rf'] = models['rf'].predict_proba(X)[:, 1]
        test_predictions['rf'] = models['rf'].predict_proba(X_test)[:, 1]

    # Logistic Regression (ìŠ¤ì¼€ì¼ë§ í•„ìš”)
    if models['lr'] is not None and models.get('scaler') is not None:
        from sklearn.preprocessing import StandardScaler
        X_scaled = pd.DataFrame(models['scaler'].transform(X), columns=X.columns, index=X.index)
        X_test_scaled = pd.DataFrame(models['scaler'].transform(X_test), columns=X_test.columns, index=X_test.index)

        train_predictions['lr'] = models['lr'].predict_proba(X_scaled)[:, 1]
        test_predictions['lr'] = models['lr'].predict_proba(X_test_scaled)[:, 1]

    # ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì„¤ì • (ì„±ëŠ¥ ê¸°ë°˜)
    weights = {}
    total_weight = 0

    for model_name in train_predictions.keys():
        if model_name in cv_scores and len(cv_scores[model_name]) > 0:
            # CV ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜
            weights[model_name] = cv_scores[model_name].mean()
            total_weight += weights[model_name]

    # ê°€ì¤‘ì¹˜ ì •ê·œí™”
    if total_weight > 0:
        for model_name in weights:
            weights[model_name] /= total_weight
    else:
        # ëª¨ë“  ëª¨ë¸ì— ë™ì¼ ê°€ì¤‘ì¹˜
        equal_weight = 1.0 / len(train_predictions)
        weights = {name: equal_weight for name in train_predictions.keys()}

    print(f"ğŸ“Š ì•™ìƒë¸” ê°€ì¤‘ì¹˜: {weights}")

    # ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”
    ensemble_pred_train = np.zeros(len(X))
    test_ensemble_pred = np.zeros(len(X_test))

    for model_name, weight in weights.items():
        if model_name in train_predictions:
            ensemble_pred_train += weight * train_predictions[model_name]
            test_ensemble_pred += weight * test_predictions[model_name]

    # ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
    ensemble_auc = roc_auc_score(y, ensemble_pred_train)
    print(f"Ensemble ROC-AUC (Train): {ensemble_auc:.4f}")

    # ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
    print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ìš”ì•½")
    print("="*50)

    model_names = []
    mean_scores = []
    std_scores = []

    for model_name in ['xgb', 'lgb', 'rf', 'lr']:
        if model_name in cv_scores and len(cv_scores[model_name]) > 0 and cv_scores[model_name].mean() > 0.5:
            model_display_names = {
                'xgb': 'XGBoost',
                'lgb': 'LightGBM',
                'rf': 'Random Forest',
                'lr': 'Logistic Regression'
            }
            model_names.append(model_display_names[model_name])
            mean_scores.append(cv_scores[model_name].mean())
            std_scores.append(cv_scores[model_name].std())

    # Ensemble ì¶”ê°€
    model_names.append('Ensemble')
    mean_scores.append(ensemble_auc)
    std_scores.append(0.000)

    results = pd.DataFrame({
        'Model': model_names,
        'CV_ROC_AUC_Mean': mean_scores,
        'CV_ROC_AUC_Std': std_scores
    })

    print(results.round(4))

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
    print("\nğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ìƒì„±...")

    print("âœ… ì˜ˆì¸¡ ì™„ë£Œ!")

    # ì˜ˆì¸¡ ë¶„í¬ í™•ì¸
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ë¶„í¬:")
    print(f"Min: {test_ensemble_pred.min():.4f}")
    print(f"Max: {test_ensemble_pred.max():.4f}")
    print(f"Mean: {test_ensemble_pred.mean():.4f}")
    print(f"Std: {test_ensemble_pred.std():.4f}")

    return results, ensemble_pred_train, test_ensemble_pred

def analyze_feature_importance(models, X):
    """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""
    print_section("íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„")

    feature_importances = {}

    # XGBoost íŠ¹ì„± ì¤‘ìš”ë„
    if models['xgb'] is not None:
        feature_importances['xgb'] = pd.DataFrame({
            'feature': X.columns,
            'importance': models['xgb'].feature_importances_
        }).sort_values('importance', ascending=False)

    # LightGBM íŠ¹ì„± ì¤‘ìš”ë„
    if models['lgb'] is not None:
        feature_importances['lgb'] = pd.DataFrame({
            'feature': X.columns,
            'importance': models['lgb'].feature_importances_
        }).sort_values('importance', ascending=False)

    # Random Forest íŠ¹ì„± ì¤‘ìš”ë„
    if models['rf'] is not None:
        feature_importances['rf'] = pd.DataFrame({
            'feature': X.columns,
            'importance': models['rf'].feature_importances_
        }).sort_values('importance', ascending=False)

    # ì²« ë²ˆì§¸ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„ ì¶œë ¥
    primary_model = None
    if 'xgb' in feature_importances:
        primary_model = 'xgb'
        model_name = 'XGBoost'
    elif 'lgb' in feature_importances:
        primary_model = 'lgb'
        model_name = 'LightGBM'
    elif 'rf' in feature_importances:
        primary_model = 'rf'
        model_name = 'Random Forest'

    if primary_model:
        print(f"ğŸ¯ {model_name} ìƒìœ„ 15ê°œ ì¤‘ìš” íŠ¹ì„±:")
        print(feature_importances[primary_model].head(15).to_string(index=False))

    # íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
    fig_width = len(feature_importances) * 7
    fig, axes = plt.subplots(1, len(feature_importances), figsize=(fig_width, 8))

    if len(feature_importances) == 1:
        axes = [axes]

    for i, (model_key, importance_df) in enumerate(feature_importances.items()):
        model_display_names = {
            'xgb': 'ğŸš€ XGBoost',
            'lgb': 'âš¡ LightGBM',
            'rf': 'ğŸŒ² Random Forest'
        }

        sns.barplot(data=importance_df.head(15), x='importance', y='feature', ax=axes[i])
        axes[i].set_title(f'{model_display_names[model_key]} Feature Importance (Top 15)')
        axes[i].set_xlabel('Importance')

    plt.tight_layout()
    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
    print("ğŸ“Š íŠ¹ì„± ì¤‘ìš”ë„ ì°¨íŠ¸ ì €ì¥ë¨: feature_importance.png")
    plt.show()

    # ì£¼ìš” íŠ¹ì„± ì¤‘ìš”ë„ ë°˜í™˜ (ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸)
    if primary_model:
        return feature_importances[primary_model]
    else:
        print("âš ï¸ íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame({'feature': X.columns, 'importance': [0] * len(X.columns)})
    plt.show()

    # ì£¼ìš” íŠ¹ì„± ì¤‘ìš”ë„ ë°˜í™˜ (ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸)
    if primary_model:
        return feature_importances[primary_model]
    else:
        print("âš ï¸ íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame({'feature': X.columns, 'importance': [0] * len(X.columns)})
    plt.show()

    return feature_importance_xgb

def create_submission(test, test_predictions):
    """ì œì¶œ íŒŒì¼ ìƒì„±"""
    print_section("ì œì¶œ íŒŒì¼ ìƒì„±")

    # ì œì¶œ íŒŒì¼ ìƒì„±
    submission = pd.DataFrame({
        'id': test['id'],
        'Exited': test_predictions
    })

    # ì œì¶œ íŒŒì¼ ê²€ì¦
    print(f"ğŸ“‹ ì œì¶œ íŒŒì¼ ê²€ì¦:")
    print(f"Shape: {submission.shape}")
    print(f"ID ë²”ìœ„: {submission['id'].min()} ~ {submission['id'].max()}")
    print(f"ì˜ˆì¸¡ê°’ ë²”ìœ„: {submission['Exited'].min():.4f} ~ {submission['Exited'].max():.4f}")
    print(f"ê²°ì¸¡ì¹˜: {submission.isnull().sum().sum()}")

    # ì œì¶œ íŒŒì¼ ì €ì¥
    submission.to_csv('submission.csv', index=False)
    print(f"\nâœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: submission.csv")

    # ìƒ˜í”Œ í™•ì¸
    print(f"\nğŸ“‹ ì œì¶œ íŒŒì¼ ìƒ˜í”Œ (ì²˜ìŒ 10í–‰):")
    print(submission.head(10))

    return submission

def business_insights_analysis(train_fe, y, ensemble_pred_train, overall_churn_rate):
    """ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë¶„ì„"""
    print_section("ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë¶„ì„")

    # ê³ ìœ„í—˜ ê³ ê° ì‹ë³„ (ìƒìœ„ 10%)
    high_risk_threshold = np.percentile(ensemble_pred_train, 90)
    high_risk_mask = ensemble_pred_train > high_risk_threshold
    high_risk_customers = train_fe[high_risk_mask]

    print(f"ğŸš¨ ê³ ìœ„í—˜ ê³ ê° ë¶„ì„:")
    print(f"â€¢ ì„ê³„ê°’: {high_risk_threshold:.3f}")
    print(f"â€¢ ê³ ìœ„í—˜ ê³ ê° ìˆ˜: {high_risk_mask.sum():,}ëª… ({high_risk_mask.mean()*100:.1f}%)")

    # ê³ ìœ„í—˜ ê³ ê°ì˜ ì‹¤ì œ ì´íƒˆë¥ 
    actual_churn_in_high_risk = high_risk_customers['Exited'].mean()

    print(f"â€¢ ê³ ìœ„í—˜êµ° ì‹¤ì œ ì´íƒˆë¥ : {actual_churn_in_high_risk:.1%}")
    print(f"â€¢ ì „ì²´ í‰ê·  ì´íƒˆë¥ : {overall_churn_rate:.1%}")
    print(f"â€¢ ìœ„í—˜ë„ ì •í™•ë„: {actual_churn_in_high_risk/overall_churn_rate:.1f}ë°° í–¥ìƒ")

    # ê³ ìœ„í—˜ ê³ ê° í”„ë¡œíŒŒì¼ ë¶„ì„
    print(f"\nğŸ‘¥ ê³ ìœ„í—˜ ê³ ê° íŠ¹ì„± í”„ë¡œíŒŒì¼:")

    # ì§€ì—­ë³„ ë¶„í¬
    geo_profile = high_risk_customers['Geography'].value_counts(normalize=True)
    print(f"\nì§€ì—­ë³„ ë¶„í¬:")
    for geo, pct in geo_profile.items():
        print(f"â€¢ {geo}: {pct:.1%}")

    # ì„±ë³„ ë¶„í¬
    gender_profile = high_risk_customers['Gender'].value_counts(normalize=True)
    print(f"\nì„±ë³„ ë¶„í¬:")
    for gender, pct in gender_profile.items():
        print(f"â€¢ {gender}: {pct:.1%}")

    # ë‚˜ì´ëŒ€ ë¶„í¬
    age_groups = pd.cut(high_risk_customers['Age'], bins=[0, 30, 40, 50, 60, 100],
                        labels=['18-30', '31-40', '41-50', '51-60', '60+'])
    age_profile = age_groups.value_counts(normalize=True)
    print(f"\në‚˜ì´ëŒ€ ë¶„í¬:")
    for age_group, pct in age_profile.items():
        print(f"â€¢ {age_group}: {pct:.1%}")

    # ì œí’ˆ ìˆ˜ ë¶„í¬
    product_profile = high_risk_customers['NumOfProducts'].value_counts(normalize=True)
    print(f"\në³´ìœ  ì œí’ˆ ìˆ˜:")
    for products, pct in product_profile.items():
        print(f"â€¢ {products}ê°œ ì œí’ˆ: {pct:.1%}")

    # ìœ„í—˜ë„ë³„ ê³ ê° ì„¸ë¶„í™”
    print(f"\nğŸ“Š ìœ„í—˜ë„ë³„ ê³ ê° ì„¸ë¶„í™”:")

    # 5ë¶„ìœ„ë¡œ ë‚˜ëˆ„ê¸°
    risk_quintiles = pd.qcut(ensemble_pred_train, q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])
    segment_analysis = pd.DataFrame({
        'Risk_Segment': risk_quintiles,
        'Actual_Churn': y
    }).groupby('Risk_Segment')['Actual_Churn'].agg(['count', 'mean']).round(4)

    segment_analysis['Lift'] = segment_analysis['mean'] / overall_churn_rate

    print(segment_analysis)

    return actual_churn_in_high_risk, segment_analysis

def calculate_roi(train, overall_churn_rate, actual_churn_in_high_risk):
    """ROI ê³„ì‚°"""
    print_section("ROI ë¶„ì„")

    # ê°€ì •ê°’ë“¤
    monthly_customers = len(train)
    avg_customer_value_eur = 2500  # ê³ ê°ë‹¹ ì—°ê°„ ê°€ì¹˜
    campaign_cost_per_customer = 50  # ë¦¬í…ì…˜ ìº í˜ì¸ ë¹„ìš©
    campaign_effectiveness = 0.30  # ìº í˜ì¸ ì„±ê³µë¥ 

    # í˜„ì¬ ìƒí™© (ëª¨ë¸ ì—†ì´)
    monthly_churners = monthly_customers * overall_churn_rate
    annual_churn_loss = monthly_churners * 12 * avg_customer_value_eur

    # ëª¨ë¸ ì ìš© í›„ (ìƒìœ„ 10% íƒ€ê²ŸíŒ…)
    high_risk_customers_count = int(monthly_customers * 0.10)
    identified_churners = high_risk_customers_count * actual_churn_in_high_risk
    prevented_churn = identified_churners * campaign_effectiveness
    annual_prevented_loss = prevented_churn * 12 * avg_customer_value_eur
    annual_campaign_cost = high_risk_customers_count * 12 * campaign_cost_per_customer

    # ROI ê³„ì‚°
    net_benefit = annual_prevented_loss - annual_campaign_cost
    roi_percentage = (net_benefit / annual_campaign_cost) * 100 if annual_campaign_cost > 0 else 0

    print(f"ğŸ’° ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ë° ROI ë¶„ì„:")
    print(f"ğŸ“ˆ ì˜ˆìƒ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸:")
    print(f"â€¢ ì›” ê³ ê° ìˆ˜: {monthly_customers:,}ëª…")
    print(f"â€¢ ì›” ì˜ˆìƒ ì´íƒˆ ê³ ê°: {monthly_churners:.0f}ëª…")
    print(f"â€¢ ì—°ê°„ ì´íƒˆ ì†ì‹¤: â‚¬{annual_churn_loss:,.0f}")
    print(f"")
    print(f"ğŸ¯ ëª¨ë¸ ì ìš© íš¨ê³¼:")
    print(f"â€¢ ì›” íƒ€ê²Ÿ ê³ ê° ìˆ˜: {high_risk_customers_count:,}ëª…")
    print(f"â€¢ ì˜ˆìƒ ì´íƒˆ ë°©ì§€: {prevented_churn:.0f}ëª…/ì›”")
    print(f"â€¢ ì—°ê°„ ì†ì‹¤ ë°©ì§€ ê¸ˆì•¡: â‚¬{annual_prevented_loss:,.0f}")
    print(f"â€¢ ì—°ê°„ ìº í˜ì¸ ë¹„ìš©: â‚¬{annual_campaign_cost:,.0f}")
    print(f"â€¢ ìˆœ ì´ìµ: â‚¬{net_benefit:,.0f}")
    print(f"â€¢ ROI: {roi_percentage:.0f}%")

    return net_benefit, roi_percentage

def model_interpretation_analysis(train_fe, feature_importance_xgb):
    """ëª¨ë¸ í•´ì„ ë¶„ì„"""
    print_section("ëª¨ë¸ í•´ì„ ë¶„ì„")

    # ìƒìœ„ íŠ¹ì„±ë³„ ì´íƒˆë¥  ì˜í–¥ ë¶„ì„
    top_features = feature_importance_xgb.head(8)['feature'].tolist()
    feature_impact = {}

    for feature in top_features:
        if feature in train_fe.columns:
            feature_median = train_fe[feature].median()
            high_feature = train_fe[train_fe[feature] > feature_median]
            low_feature = train_fe[train_fe[feature] <= feature_median]

            impact = high_feature['Exited'].mean() - low_feature['Exited'].mean()
            feature_impact[feature] = impact

    print("ì£¼ìš” íŠ¹ì„±ë³„ ì´íƒˆë¥  ì˜í–¥:")
    for feature, impact in sorted(feature_impact.items(), key=lambda x: abs(x[1]), reverse=True):
        direction = "ì¦ê°€" if impact > 0 else "ê°ì†Œ"
        print(f"â€¢ {feature}: {abs(impact):.1%} {direction}")

def print_recommendations():
    """ì‹¤ë¬´ ì ìš© ê¶Œì¥ì‚¬í•­"""
    print_section("ì‹¤ë¬´ ì ìš© ê¶Œì¥ì‚¬í•­")

    recommendations = [
        "ğŸ¯ ê³ ìœ„í—˜ ê³ ê° ê´€ë¦¬ ì „ëµ:",
        "  â€¢ ì˜ˆì¸¡ ì ìˆ˜ 0.7 ì´ìƒ ê³ ê° ì›” 1íšŒ ì´ìƒ ì ‘ì´‰",
        "  â€¢ 50ì„¸ ì´ìƒ + ë‹¨ì¼ ì œí’ˆ ê³ ê° ìš°ì„  ê´€ë¦¬",
        "  â€¢ ì”ì•¡ 0ì› ê³ ê° ëŒ€ìƒ ì˜ˆê¸ˆ ìƒí’ˆ ì ê·¹ ì œì•ˆ",
        "",
        "ğŸ“ ë§ì¶¤í˜• ë¦¬í…ì…˜ ìº í˜ì¸:",
        "  â€¢ ë…ì¼ ì§€ì—­ ê³ ê° ì „ìš© ìƒí’ˆ/ì„œë¹„ìŠ¤ ê°œë°œ",
        "  â€¢ ë¹„í™œì„± íšŒì› ëŒ€ìƒ ë””ì§€í„¸ ì„œë¹„ìŠ¤ í˜œíƒ ì œê³µ",
        "  â€¢ ì‹ ìš©ì ìˆ˜ í•˜ìœ„ ê³ ê° ê¸ˆìœµ ìƒë‹´ ì„œë¹„ìŠ¤",
        "",
        "ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ê°œì„ :",
        "  â€¢ ì›”ë³„ ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§",
        "  â€¢ ìº í˜ì¸ íš¨ê³¼ A/B í…ŒìŠ¤íŠ¸",
        "  â€¢ ë¶„ê¸°ë³„ ëª¨ë¸ ì¬í›ˆë ¨"
    ]

    for rec in recommendations:
        print(rec)

def create_final_visualization(results, ensemble_pred_train, test_ensemble_pred, segment_analysis, feature_importance_df):
    """ìµœì¢… ê²°ê³¼ ì‹œê°í™”"""
    print_section("ìµœì¢… ê²°ê³¼ ì‹œê°í™”")

    try:
        # ìµœì¢… ê²°ê³¼ ì¢…í•© ì‹œê°í™”
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('ğŸ† Bank Customer Churn Prediction - Final Results', fontsize=16, y=0.98)

        # 1. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
        if results is not None and len(results) > 0:
            bars = axes[0,0].bar(results['Model'], results['CV_ROC_AUC_Mean'],
                                 yerr=results['CV_ROC_AUC_Std'], capsize=5, alpha=0.7,
                                 color=['skyblue', 'lightgreen', 'salmon', 'gold'][:len(results)])
            axes[0,0].set_title('ğŸ† Model Performance Comparison')
            axes[0,0].set_ylabel('ROC-AUC Score')
            axes[0,0].set_ylim(0.5, 1.0)
            for i, (model, score) in enumerate(zip(results['Model'], results['CV_ROC_AUC_Mean'])):
                axes[0,0].text(i, score + 0.01, f'{score:.4f}', ha='center', va='bottom', fontweight='bold')
        else:
            axes[0,0].text(0.5, 0.5, 'No model results available', ha='center', va='center', transform=axes[0,0].transAxes)
            axes[0,0].set_title('ğŸ† Model Performance Comparison')

        # 2. ì˜ˆì¸¡ ë¶„í¬
        if ensemble_pred_train is not None and test_ensemble_pred is not None:
            axes[0,1].hist(ensemble_pred_train, bins=50, alpha=0.7, label='Training Predictions', color='blue')
            axes[0,1].hist(test_ensemble_pred, bins=50, alpha=0.7, label='Test Predictions', color='red')
            axes[0,1].set_title('ğŸ”® Prediction Distribution')
            axes[0,1].set_xlabel('Churn Probability')
            axes[0,1].set_ylabel('Frequency')
            axes[0,1].legend()
        else:
            axes[0,1].text(0.5, 0.5, 'No prediction data available', ha='center', va='center', transform=axes[0,1].transAxes)
            axes[0,1].set_title('ğŸ”® Prediction Distribution')

        # 3. ìœ„í—˜ë„ë³„ ê³ ê° ì„¸ë¶„í™”
        if segment_analysis is not None and len(segment_analysis) > 0:
            segment_analysis_plot = segment_analysis.reset_index()
            bars = axes[1,0].bar(segment_analysis_plot['Risk_Segment'], segment_analysis_plot['mean'],
                                 alpha=0.7, color='orange')
            axes[1,0].set_title('ğŸ“Š Risk Segment Analysis')
            axes[1,0].set_ylabel('Actual Churn Rate')
            axes[1,0].set_xlabel('Risk Segment')
            for bar, rate in zip(bars, segment_analysis_plot['mean']):
                axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                               f'{rate:.2%}', ha='center', va='bottom')
        else:
            axes[1,0].text(0.5, 0.5, 'No segment analysis available', ha='center', va='center', transform=axes[1,0].transAxes)
            axes[1,0].set_title('ğŸ“Š Risk Segment Analysis')

        # 4. ìƒìœ„ íŠ¹ì„± ì¤‘ìš”ë„
        if feature_importance_df is not None and len(feature_importance_df) > 0:
            top_10_features = feature_importance_df.head(10)
            if len(top_10_features) > 0:
                axes[1,1].barh(range(len(top_10_features)), top_10_features['importance'], alpha=0.7, color='green')
                axes[1,1].set_yticks(range(len(top_10_features)))
                axes[1,1].set_yticklabels(top_10_features['feature'])
                axes[1,1].set_title('ğŸ¯ Top 10 Feature Importance')
                axes[1,1].set_xlabel('Importance')
            else:
                axes[1,1].text(0.5, 0.5, 'No feature importance data', ha='center', va='center', transform=axes[1,1].transAxes)
                axes[1,1].set_title('ğŸ¯ Top 10 Feature Importance')
        else:
            axes[1,1].text(0.5, 0.5, 'No feature importance data', ha='center', va='center', transform=axes[1,1].transAxes)
            axes[1,1].set_title('ğŸ¯ Top 10 Feature Importance')

        plt.tight_layout()
        plt.savefig('final_results.png', dpi=300, bbox_inches='tight')
        print("ğŸ“Š ìµœì¢… ê²°ê³¼ ì°¨íŠ¸ ì €ì¥ë¨: final_results.png")
        plt.show()

    except Exception as e:
        print(f"âš ï¸ ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ì‹œê°í™”ë¥¼ ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")

def print_final_summary(train, test, results, overall_churn_rate, actual_churn_in_high_risk, net_benefit, roi_percentage):
    """ìµœì¢… ìš”ì•½ ì¶œë ¥"""
    print_header("ìµœì¢… ë¶„ì„ ê²°ê³¼ ìš”ì•½")

    # ì•ˆì „í•œ ê²°ê³¼ ì²˜ë¦¬
    best_performance = "N/A"
    model_scores = {}

    if results is not None and len(results) > 0:
        best_performance = f"{results['CV_ROC_AUC_Mean'].max():.4f}"

        for _, row in results.iterrows():
            model_scores[row['Model']] = f"{row['CV_ROC_AUC_Mean']:.4f}"

    final_summary = f"""
ğŸ“Š ë°ì´í„° ë¶„ì„ ê²°ê³¼:
â€¢ ì´ ê³ ê° ìˆ˜: {len(train):,}ëª… (í›ˆë ¨) + {len(test):,}ëª… (í…ŒìŠ¤íŠ¸)
â€¢ ì „ì²´ ì´íƒˆë¥ : {overall_churn_rate:.1%}
â€¢ ì£¼ìš” ì´íƒˆ ìš”ì¸: ë‚˜ì´, ì”ì•¡, ì œí’ˆ ìˆ˜, í™œì„±ë„, ì§€ì—­

ğŸ¯ ëª¨ë¸ ì„±ëŠ¥:
â€¢ ìµœê³  ì„±ëŠ¥: {best_performance} (ì•™ìƒë¸” ëª¨ë¸)"""

    # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ì¶”ê°€
    for model, score in model_scores.items():
        if model != 'Ensemble':
            final_summary += f"\nâ€¢ {model}: {score}"

    final_summary += f"""

ğŸ­ ì•™ìƒë¸” êµ¬ì„±:
â€¢ ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©
â€¢ Robustí•˜ê³  ì•ˆì •ì ì¸ ì˜ˆì¸¡ ì„±ëŠ¥

ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜:
â€¢ ê³ ìœ„í—˜ ê³ ê° ì‹ë³„ ì •í™•ë„: {actual_churn_in_high_risk:.1%}
â€¢ ì˜ˆìƒ ì—°ê°„ ìˆœì´ìµ: â‚¬{net_benefit:,.0f}
â€¢ ROI: {roi_percentage:.0f}%
â€¢ ìœ„í—˜ë„ ê°œì„  íš¨ê³¼: {actual_churn_in_high_risk/overall_churn_rate:.1f}ë°°

ğŸš€ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:
â€¢ 50ì„¸ ì´ìƒ ê³ ê°ì˜ ì´íƒˆ ìœ„í—˜ í˜„ì €íˆ ë†’ìŒ
â€¢ ì”ì•¡ 0ì› ê³ ê° ì§‘ì¤‘ ê´€ë¦¬ í•„ìš”
â€¢ ë…ì¼ ì§€ì—­ íŠ¹í™” ì„œë¹„ìŠ¤ ê°œë°œ ê¶Œì¥
â€¢ ë‹¨ì¼ ì œí’ˆ ê³ ê° ëŒ€ìƒ ì¶”ê°€ ìƒí’ˆ ì œì•ˆ

ğŸ“ˆ ë‹¤ìŒ ë‹¨ê³„:
â€¢ Kaggle ì œì¶œ ë° Public/Private LB í™•ì¸
â€¢ A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ì‹¤ì œ íš¨ê³¼ ê²€ì¦
â€¢ ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ êµ¬ì¶•
â€¢ ì •ê¸°ì  ëª¨ë¸ ì—…ë°ì´íŠ¸ ì²´ê³„ êµ¬ì¶•
"""

    print(final_summary)

    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! ì œì¶œ íŒŒì¼ ì¤€ë¹„ë¨: submission.csv")
    print(f"ğŸ“ ì˜ˆìƒ Kaggle ì„±ëŠ¥: Public LB 0.85+ ëª©í‘œ")
    print(f"ğŸ‰ Bank Customer Churn Prediction Analysis ì™„ë£Œ!")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # 1. ë°ì´í„° ë¡œë”©
        train, test, sample_submission = load_data()
        if train is None:
            return

        # 2. ê¸°ë³¸ ë°ì´í„° ë¶„ì„
        overall_churn_rate = basic_data_analysis(train, test)

        # 3. EDA ìˆ˜í–‰
        create_comprehensive_eda(train)

        # 4. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
        train_fe, test_fe = feature_engineering_pipeline(train, test)

        # 5. ëª¨ë¸ë§ ë°ì´í„° ì¤€ë¹„
        X, y = prepare_modeling_data(train_fe, 'Exited')
        X_test = prepare_modeling_data(test_fe)

        print(f"\nğŸ¯ ëª¨ë¸ë§ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
        print(f"Features shape: {X.shape}")
        print(f"Target shape: {y.shape}")
        print(f"Test features shape: {X_test.shape}")

        # 6. ëª¨ë¸ í›ˆë ¨
        models, cv_scores = train_models(X, y)

        # 7. ì•™ìƒë¸” ë° ì˜ˆì¸¡
        results, ensemble_pred_train, test_ensemble_pred = create_ensemble_and_predict(
            models, cv_scores, X, X_test, y)

        # 8. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        feature_importance_df = analyze_feature_importance(models, X)

        # 9. ì œì¶œ íŒŒì¼ ìƒì„±
        submission = create_submission(test, test_ensemble_pred)

        # 10. ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë¶„ì„
        actual_churn_in_high_risk, segment_analysis = business_insights_analysis(
            train_fe, y, ensemble_pred_train, overall_churn_rate)

        # 11. ROI ê³„ì‚°
        net_benefit, roi_percentage = calculate_roi(train, overall_churn_rate, actual_churn_in_high_risk)

        # 12. ëª¨ë¸ í•´ì„
        model_interpretation_analysis(train_fe, feature_importance_df)

        # 13. ê¶Œì¥ì‚¬í•­
        print_recommendations()

        # 14. ìµœì¢… ì‹œê°í™”
        create_final_visualization(results, ensemble_pred_train, test_ensemble_pred,
                                   segment_analysis, feature_importance_df)

        # 15. ìµœì¢… ìš”ì•½
        print_final_summary(train, test, results, overall_churn_rate,
                            actual_churn_in_high_risk, net_benefit, roi_percentage)

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        print("\nğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        print("ğŸ“Š ìµœì¢… ì œì¶œ íŒŒì¼: submission.csv")
        print("ğŸ¯ ëª©í‘œ: Kaggle Playground Series S4E1 ìƒìœ„ ë­í‚¹!")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()